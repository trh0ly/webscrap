{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© Thomas Robert Holy 2019\n",
    "<br>\n",
    "Version 0.0.1\n",
    "<br><br>\n",
    "Visit me on GitHub: https://github.com/trh0ly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# Package Import\n",
    "\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# Prüfe die Website auf Verfügbarkeit und speichere die verfügbaren\n",
    "# \"Schlüsselzahlen\" in der Liste \"good_pages\"\n",
    "\n",
    "#----------------------------------------\n",
    "# Definition der Funktion zur Seiten-Prüfung\n",
    "\n",
    "good_pages = []\n",
    "def find_good_pages(url_addon):\n",
    "    url = \"https://ranking.zeit.de/che/de/ort/\" + str(url_addon)\n",
    "    try:\n",
    "        good_page = urlopen(url)\n",
    "        good_pages.append(url_addon)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"Please Wait.. The url check will take some time\")\n",
    "\n",
    "#----------------------------------------\n",
    "# Führe die Aktion für Seite 1 bis 350 \n",
    "# durch\n",
    "\n",
    "for i in range(1,430+1):\n",
    "    find_good_pages(i)\n",
    "    if i == 430:\n",
    "        print('Ready')\n",
    "print('The \"good_pages\" are: ' + str(good_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "# \"Gute\" Seiten\n",
    "\n",
    "#good_pages = [1, 2, 3, 4, 5, 6, 7, 11, 12, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 100, 101, 102, 103, 105, 106, 107, 108, 109, 110, 111, 112, 113, 121, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 151, 152, 154, 155, 156, 157, 158, 159, 160, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 190, 191, 192, 194, 195, 196, 201, 203, 204, 205, 207, 208, 210, 211, 212, 213, 214, 216, 220, 221, 222, 224, 225, 226, 227, 228, 229, 231, 232, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 252, 253, 254, 255, 256, 260, 263, 264, 270, 280, 281, 282, 290, 291, 293, 294, 295, 297, 298, 299, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Working on the next Step...')\n",
    "print('\\n')\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Extraktion der relevanten Daten aus den jeweiligen sEiten\n",
    "\n",
    "#----------------------------------------\n",
    "# Öffne das File in dem die Daten gespeichert\n",
    "# werden sollen\n",
    "\n",
    "text_file = open(\"output.txt\",\"w\")\n",
    "\n",
    "#----------------------------------------\n",
    "# Extraktionsvorgang für jede \"gute\" Seite\n",
    "\n",
    "for good_page in sorted(good_pages):\n",
    "\n",
    "    # Aktuelle url definieren\n",
    "    url = \"https://ranking.zeit.de/che/de/ort/\" + str(good_page)\n",
    "    print('Aktuelle URL: ' + str(url))\n",
    "\n",
    "    # Öffne die url\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, 'lxml')\n",
    "\n",
    "    # Lade den Titel zur url\n",
    "    title = soup.title\n",
    "    print('Aktueller Titel: ' + str(title))\n",
    "\n",
    "    # Scanne nach Tabellen \n",
    "    rows = soup.find_all('table')\n",
    "    #print(rows)\n",
    "\n",
    "    # Wandle die Tabelleninhalte in einen String um\n",
    "    # und extrahiere den Text\n",
    "    str_cells = str(rows)\n",
    "    cleanstring = BeautifulSoup(str_cells, \"lxml\").get_text()\n",
    "    #print(cleanstring)\n",
    "\n",
    "    #----------------------------------------\n",
    "    # Enferne störende Zeichen\n",
    "\n",
    "    cleanstring = cleanstring.replace('%', '')\n",
    "    cleanstring = cleanstring.replace(']', '')\n",
    "    cleanstring = cleanstring.replace('[', '')\n",
    "    cleanstring = cleanstring.replace('*', '')\n",
    "    cleanstring = cleanstring.replace('\\r', '')\n",
    "    cleanstring = cleanstring.replace(\" ,\", '')\n",
    "    cleanstring = cleanstring.replace(\", \", '')\n",
    "    cleanstring = cleanstring.replace(\" \", '')\n",
    "    cleanstring = cleanstring.replace(\"€\", '')\n",
    "\n",
    "    #----------------------------------------\n",
    "    # Separiere einzelne Wörter und Nummern nun\n",
    "    # in separate Strings und speichere diese in \n",
    "    # einer Liste\n",
    "\n",
    "    cleaned_string = re.sub('['+string.punctuation+']', '', cleanstring).split() \n",
    "    #print(cleaned_string)\n",
    "\n",
    "    # Extrahiere alle Zahlen aus dieser Liste und speichere sie in \n",
    "    # der Liste \"pure_zahlen\"    \n",
    "    _re_digits = re.compile(r\"(-?(?:(?:\\d+(?:\\.\\d*)?)|(?:\\.\\d+)))\")\n",
    "    pure_zahlen = []\n",
    "    for element in cleaned_string:\n",
    "        pure_zahlen += [ float(n) for n in _re_digits.findall(element)]\n",
    "    #print(pure_zahlen)\n",
    "\n",
    "    # Extrahiere alle Wörter aus dieser Liste und speichere sie in \n",
    "    # der Liste \"pure_text\"    \n",
    "    pure_text = [x for x in cleaned_string if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())]\n",
    "    #print(pure_text)\n",
    "    \n",
    "    # Erstelle ein Dictonary das den beiden Keywords 'Kategorie' und 'Wert' die \n",
    "    # entsprechenden Listen zuordnet, welches grundlage für den DataFrame \"table\"\n",
    "    # ist in welchem alle Informationen in tabellenfrom aufgeführt werden. \n",
    "    # Nicht vorhandene Einträge werden mit \"NaN\" gekennzeichnet\n",
    "    my_dict = {'Kategorie': np.array(pure_text), 'Wert': np.array(pure_zahlen)}\n",
    "    table = pd.concat([pd.Series(v, name=k) for k, v in my_dict.items()], axis=1)\n",
    "    #print(table)\n",
    "    #print('\\n')\n",
    "    \n",
    "    # Speichere die relevanten Inoformationen mit ggf. \n",
    "    # Umbrüchen in dem File\n",
    "    text_file.write(str(title))\n",
    "    text_file.write(str(url))\n",
    "    text_file.write('\\n')\n",
    "    text_file.write(str(table))\n",
    "    text_file.write('\\n')\n",
    "    text_file.write('\\n')    \n",
    "\n",
    "#----------------------------------------\n",
    "# File schließen\n",
    "\n",
    "text_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
