{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web-Scraping - Informationen von Websites downloaden\n",
    "© Thomas Robert Holy 2019\n",
    "<br>\n",
    "Version 1.0\n",
    "<br><br>\n",
    "Visit me on GitHub: https://github.com/trh0ly\n",
    "\n",
    "## Grundlegende Einstellungen:\n",
    "### Import von Modulen\n",
    "Zunächst müssen die notwendigen Module importiert werden, damit auf diese zugegriffen werden kann. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prüfung der Websites\n",
    "Nun müssen die Websites im Ranking systematisch überprüft werden, da einzelne Seiten zwischendurch nicht aufrufbar sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# Prüfe die Website auf Verfügbarkeit und speichere die verfügbaren\n",
    "# \"Schlüsselzahlen\" in der Liste \"good_pages\"\n",
    "\n",
    "#----------------------------------------\n",
    "# Definition der Funktion zur Seiten-Prüfung\n",
    "\n",
    "good_pages = []\n",
    "def find_good_pages(url_addon):\n",
    "    url = \"https://ranking.zeit.de/che/de/ort/\" + str(url_addon)\n",
    "    try:\n",
    "        good_page = urlopen(url)\n",
    "        good_pages.append(url_addon)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"Please Wait.. The url check will take some time\")\n",
    "\n",
    "#----------------------------------------\n",
    "# Führe die Aktion für Seite 1 bis 350 durch\n",
    "\n",
    "for i in range(1,430+1):\n",
    "    find_good_pages(i)\n",
    "    if i == 430:\n",
    "        print('Ready')\n",
    "print('The \"good_pages\" are: ' + str(good_pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download der Informationen und Speicherung\n",
    "Im nächsten Schritt werden die validen Websites nach Informationen durchsucht.\n",
    "Diese werden dann anschließend erschlossen und aufbereitet.\n",
    "Zum Schluss werden die so extrahierten Informationen im einer Text.Datei gespeichert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Working on the next Step... \\n')\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Extraktion der relevanten Daten aus den jeweiligen sEiten\n",
    "\n",
    "#----------------------------------------\n",
    "# Öffne das File in dem die Daten gespeichert\n",
    "# werden sollen\n",
    "\n",
    "text_file = open(\"output.txt\",\"w\")\n",
    "\n",
    "#----------------------------------------\n",
    "# Extraktionsvorgang für jede \"gute\" Seite\n",
    "\n",
    "for good_page in sorted(good_pages):\n",
    "\n",
    "    # Aktuelle url definieren\n",
    "    url = \"https://ranking.zeit.de/che/de/ort/\" + str(good_page)\n",
    "    print('Aktuelle URL: ' + str(url))\n",
    "\n",
    "    # Öffne die url\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, 'lxml')\n",
    "\n",
    "    # Lade den Titel zur url\n",
    "    title = soup.title\n",
    "    print('Aktueller Titel: ' + str(title))\n",
    "\n",
    "    # Scanne nach Tabellen \n",
    "    rows = soup.find_all('table')\n",
    "    #print(rows)\n",
    "\n",
    "    # Wandle die Tabelleninhalte in einen String um\n",
    "    # und extrahiere den Text\n",
    "    str_cells = str(rows)\n",
    "    cleanstring = BeautifulSoup(str_cells, \"lxml\").get_text()\n",
    "    #print(cleanstring)\n",
    "\n",
    "    #----------------------------------------\n",
    "    # Enferne störende Zeichen\n",
    "\n",
    "    cleanstring = cleanstring.replace('%', '')\n",
    "    cleanstring = cleanstring.replace(']', '')\n",
    "    cleanstring = cleanstring.replace('[', '')\n",
    "    cleanstring = cleanstring.replace('*', '')\n",
    "    cleanstring = cleanstring.replace('\\r', '')\n",
    "    cleanstring = cleanstring.replace(\" ,\", '')\n",
    "    cleanstring = cleanstring.replace(\", \", '')\n",
    "    cleanstring = cleanstring.replace(\" \", '')\n",
    "    cleanstring = cleanstring.replace(\"€\", '')\n",
    "\n",
    "    #----------------------------------------\n",
    "    # Separiere einzelne Wörter und Nummern nun\n",
    "    # in separate Strings und speichere diese in \n",
    "    # einer Liste\n",
    "\n",
    "    cleaned_string = re.sub('['+string.punctuation+']', '', cleanstring).split() \n",
    "    #print(cleaned_string)\n",
    "\n",
    "    # Extrahiere alle Zahlen aus dieser Liste und speichere sie in \n",
    "    # der Liste \"pure_zahlen\"    \n",
    "    _re_digits = re.compile(r\"(-?(?:(?:\\d+(?:\\.\\d*)?)|(?:\\.\\d+)))\")\n",
    "    pure_zahlen = []\n",
    "    for element in cleaned_string:\n",
    "        pure_zahlen += [ float(n) for n in _re_digits.findall(element)]\n",
    "    #print(pure_zahlen)\n",
    "\n",
    "    # Extrahiere alle Wörter aus dieser Liste und speichere sie in \n",
    "    # der Liste \"pure_text\"    \n",
    "    pure_text = [x for x in cleaned_string if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())]\n",
    "    #print(pure_text)\n",
    "    \n",
    "    # Erstelle ein Dictonary das den beiden Keywords 'Kategorie' und 'Wert' die \n",
    "    # entsprechenden Listen zuordnet, welches grundlage für den DataFrame \"table\"\n",
    "    # ist in welchem alle Informationen in tabellenfrom aufgeführt werden. \n",
    "    # Nicht vorhandene Einträge werden mit \"NaN\" gekennzeichnet\n",
    "    my_dict = {'Kategorie': np.array(pure_text), 'Wert': np.array(pure_zahlen)}\n",
    "    table = pd.concat([pd.Series(v, name=k) for k, v in my_dict.items()], axis=1)\n",
    "    #print(table)\n",
    "    #print('\\n')\n",
    "    \n",
    "    # Speichere die relevanten Inoformationen mit ggf. \n",
    "    # Umbrüchen in dem File\n",
    "    text_file.write(str(title))\n",
    "    text_file.write(str(url))\n",
    "    text_file.write('\\n')\n",
    "    text_file.write(str(table))\n",
    "    text_file.write('\\n')\n",
    "    text_file.write('\\n')    \n",
    "\n",
    "#----------------------------------------\n",
    "# File schließen\n",
    "\n",
    "text_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
